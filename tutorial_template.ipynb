{
 "cells": [
 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template structure for making RISE MICCAI tutorials.\n",
"We suggest to follow this structure to ensure that all tutorials have a common structure."]
  },
 


{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìì [Tutorial Title: e.g., \"Lung Nodule Detection with PyTorch\"]\n",
    "\n",
    "**Creator:** [Your Name]  \n",
    "**Date:** [Date of Creation/Last Update]  \n",
    "**Video Link:** [Link to YouTube Video Tutorial] this will be added later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü 1. Title & Overview\n",
    "\n",
    "<!-- \n",
    "* **Concise title:** Make it catchy and informative.\n",
    "* **Short description:** 1-2 sentences summarizing the tutorial's content and goal.\n",
    "* **Learning objectives:** Use bullet points for clarity. What will the user be able to do after completing this tutorial?\n",
    "-->\n",
    "\n",
    "**Title:** `[Insert Concise Title of the Tutorial Here]`\n",
    "\n",
    "**Description:**\n",
    "`[Insert a brief (1-2 sentences) description of what this tutorial covers. e.g., \"This tutorial walks you through the process of building and training a convolutional neural network (CNN) to detect lung nodules in CT scans using Python, PyTorch, and a publicly available dataset.\"]`\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this tutorial, you will be able to:\n",
    "* Understand `[Concept 1, e.g., the basics of medical image analysis for nodule detection]`\n",
    "* Implement `[Technique 1, e.g., a data loading pipeline for DICOM images]`\n",
    "* Build `[Model/Tool, e.g., a simple CNN model for binary classification]`\n",
    "* Evaluate `[Skill, e.g., the performance of your model using appropriate metrics]`\n",
    "* Visualize `[Output, e.g., model predictions on sample images]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 2. Introduction\n",
    "\n",
    "<!--\n",
    "* **Background:** Provide context. What is the problem you're solving?\n",
    "* **Importance:** Why is this specific topic/task significant in medical imaging?\n",
    "* **Real-world use cases:** Give concrete examples of how this is applied.\n",
    "* Keep this section engaging and motivate the reader.\n",
    "-->\n",
    "\n",
    "`[Provide background information on the topic. Explain the problem domain, e.g., \"Lung cancer is a leading cause of cancer-related deaths worldwide. Early detection through screening programs, often involving CT scans, is crucial for improving patient outcomes. Identifying suspicious lung nodules in these scans is a key step...\"]`\n",
    "\n",
    "**Why is this important in medical imaging?**\n",
    "`[Explain the significance. e.g., \"Automating or assisting in the detection of lung nodules can help radiologists by reducing their workload, improving detection consistency, and potentially identifying subtle nodules that might be missed...\"]`\n",
    "\n",
    "**Real-world use cases and applications:**\n",
    "* `[Use Case 1, e.g., Computer-Aided Detection (CAD) systems in hospitals]`\n",
    "* `[Use Case 2, e.g., Assisting in large-scale screening programs]`\n",
    "* `[Use Case 3, e.g., Quantitative analysis of nodule characteristics over time]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Pre-requisites\n",
    "\n",
    "<!--\n",
    "* **Libraries:** List all necessary Python libraries. Provide pip install commands.\n",
    "* **Dataset:** Specify the dataset.\n",
    "* **Dataset Link & Description:** Provide a direct link and a brief overview of the dataset (source, type of images, annotations, etc.).\n",
    "* **Download & Preparation:** Step-by-step instructions on how to download and prepare the dataset. This might include unzipping, organizing files, or running a preprocessing script.\n",
    "-->\n",
    "\n",
    "**Required Python Libraries:**\n",
    "Make sure you have the following libraries installed. You can install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib scikit-learn opencv-python pydicom torch torchvision torcheval tqdm\n",
    "# !pip install [any-other-specific-library]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset to Use:**\n",
    "* **Name:** `[e.g., LUNA16 (Lung Nodule Analysis 16)]`\n",
    "* **Link:** `[Provide a direct URL to the dataset or its official page]`\n",
    "* **Description:** `[Briefly describe the dataset: e.g., \"The LUNA16 dataset contains chest CT scans from the LIDC-IDRI database, with annotations for lung nodules. It's widely used for developing and evaluating nodule detection algorithms.\"]`\n",
    "\n",
    "**How to Download & Prepare the Dataset:**\n",
    "1.  `[Step 1: e.g., Download the dataset from the provided link. You might need to register.]`\n",
    "2.  `[Step 2: e.g., Extract the downloaded files to a specific directory, e.g., './data/LUNA16']`\n",
    "3.  `[Step 3: e.g., Run any provided preprocessing scripts or detail manual steps for organizing files if needed. For example, converting .mhd/.raw to .npy or .png if preferred for easier loading, or creating a CSV file mapping image paths to labels.]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Include a small Python snippet here if there's a simple script for preparation\n",
    "# import os\n",
    "# def prepare_dataset(raw_data_path, processed_data_path):\n",
    "#     # Your preparation code here\n",
    "#     print(\"Dataset preparation complete.\")\n",
    "# prepare_dataset('./data/LUNA16_raw', './data/LUNA16_processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª 4. Hands-On Code Implementation\n",
    "\n",
    "<!--\n",
    "* **Structure:** Break this section into logical sub-sections (as listed).\n",
    "* **Code Blocks:** Use clear, well-commented code blocks.\n",
    "* **Markdown Explanations:** Precede each code block with a Markdown cell explaining *what* the code does and *why*.\n",
    "* **Step-by-step:** Guide the user through each part of the implementation.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Importing Necessary Libraries\n",
    "\n",
    "We'll start by importing all the Python libraries we'll need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing and medical imaging\n",
    "import cv2 # OpenCV for image manipulation\n",
    "import pydicom # For reading DICOM files\n",
    "# from PIL import Image # If using PIL\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.tensorboard import SummaryWriter # For TensorBoard logging\n",
    "\n",
    "# Scikit-learn for metrics and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# For segmentation tasks, you might use:\n",
    "# from sklearn.metrics import jaccard_score # (Dice is often custom or from other libs)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loading the Dataset\n",
    "\n",
    "Now, let's define how we'll load our medical images and their corresponding labels. For PyTorch, we typically create a custom `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (adjust as necessary)\n",
    "DATASET_DIR = './data/LUNA16_processed/' # Or wherever you stored it\n",
    "METADATA_CSV = os.path.join(DATASET_DIR, 'metadata.csv') # Assuming you have/create one\n",
    "\n",
    "# Example: Custom PyTorch Dataset Class\n",
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame with image IDs and labels.\n",
    "            image_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_files = dataframe['image_id'].values\n",
    "        self.labels = dataframe['label'].values # Or masks for segmentation\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        \n",
    "        # Example: Loading a .npy file (adjust for .dcm, .png, etc.)\n",
    "        image = np.load(img_name).astype(np.float32) \n",
    "        # If 2D slices from 3D scan, image might be (H, W) or (H, W, C)\n",
    "        # If 3D scan, image might be (D, H, W)\n",
    "        \n",
    "        # Add channel dimension if it's grayscale and model expects channels (e.g., for ResNet)\n",
    "        if image.ndim == 2: # (H, W) -> (1, H, W)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "        elif image.ndim == 3 and image.shape[0] != 1 and image.shape[0] != 3 : # (D,H,W) -> (1,D,H,W) for 3D CNN or select slice\n",
    "             # For 3D CNN, it might be image = np.expand_dims(image, axis=0)\n",
    "             # Or, if you're taking a middle slice for a 2D CNN from a 3D volume:\n",
    "             # mid_slice_idx = image.shape[0] // 2\n",
    "             # image = image[mid_slice_idx, :, :] \n",
    "             # image = np.expand_dims(image, axis=0) # Add channel dim\n",
    "             pass # Adjust based on your data and model\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        # Convert label to tensor, ensure correct dtype\n",
    "        label = torch.tensor(label, dtype=torch.float32) # For BCEWithLogitsLoss\n",
    "        # label = torch.tensor(label, dtype=torch.long) # For CrossEntropyLoss\n",
    "\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            # Note: some transforms expect PIL Image or tensor (H,W,C) or (C,H,W)\n",
    "            # Ensure your loaded image format matches transform expectations.\n",
    "            # If image is (1, D, H, W) for 3D, transforms might need adjustment or custom implementation.\n",
    "\n",
    "        return sample['image'], sample['label'] # Return image and label directly for DataLoader\n",
    "\n",
    "# Load metadata (assuming you have a CSV with image_id and label)\n",
    "# This is a placeholder. You'll need to create this based on your dataset.\n",
    "# Example metadata.csv structure:\n",
    "# image_id,label\n",
    "# image_001.npy,1\n",
    "# image_002.npy,0\n",
    "try:\n",
    "    metadata_df = pd.read_csv(METADATA_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {METADATA_CSV} not found. Please create it or adjust the path.\")\n",
    "    # Create a dummy dataframe for demonstration if file not found\n",
    "    metadata_df = pd.DataFrame({\n",
    "        'image_id': [f'dummy_image_{i:03d}.npy' for i in range(100)],\n",
    "        'label': [random.randint(0,1) for _ in range(100)]\n",
    "    })\n",
    "    # You would need to actually save dummy .npy files for this to run\n",
    "    # for fname in metadata_df['image_id']:\n",
    "    #    np.save(os.path.join(DATASET_DIR, fname), np.random.rand(1, 64, 64)) # Example 2D slice\n",
    "\n",
    "# Split dataset: train, validation, test\n",
    "train_df, temp_df = train_test_split(metadata_df, test_size=0.3, random_state=42, stratify=metadata_df['label'] if 'label' in metadata_df.columns and len(metadata_df['label'].unique()) > 1 else None)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'] if 'label' in temp_df.columns and len(temp_df['label'].unique()) > 1 else None)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Preprocessing Steps\n",
    "\n",
    "Preprocessing is crucial for good model performance. This includes normalization, resizing, and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "# Adjust mean/std if you calculate them from your specific dataset\n",
    "# These are often ImageNet mean/std, which might not be optimal for medical images\n",
    "# For grayscale, mean/std would be single values.\n",
    "# For 3D images, transforms might need to be custom.\n",
    "# This example assumes 2D images that are already (C, H, W) or can be converted.\n",
    "\n",
    "# Example for 2D images (e.g. single slices)\n",
    "# If your images are (H,W) numpy arrays, ToTensor() will convert to (1,H,W) tensors\n",
    "# If they are (H,W,C) numpy arrays, ToTensor() will convert to (C,H,W) tensors\n",
    "# If they are PIL images, ToTensor() handles it.\n",
    "\n",
    "# Note: If your MedicalImageDataset returns numpy arrays, ensure they are in a format\n",
    "# that ToTensor() and other transforms expect. Typically (H, W, C) or (H, W).\n",
    "# If your data is already (C,H,W) tensor, you might not need ToTensor().\n",
    "\n",
    "# For this example, let's assume MedicalImageDataset provides a (1, H, W) numpy array\n",
    "# and we convert it to tensor within the dataset or here.\n",
    "# If it's already a tensor, skip ToTensor().\n",
    "\n",
    "# Let's assume images are loaded as numpy arrays (e.g. (64,64) or (1,64,64))\n",
    "# and need to be converted to tensors and normalized.\n",
    "# If images are already (C,H,W) tensors, transforms.Normalize is fine.\n",
    "# If they are numpy arrays, you'd typically include transforms.ToTensor() first.\n",
    "\n",
    "# Let's refine the dataset's __getitem__ to output a tensor directly,\n",
    "# so transforms here are simpler.\n",
    "# (Revisiting MedicalImageDataset's __getitem__ for clarity with transforms)\n",
    "# class MedicalImageDataset(...):\n",
    "#     def __getitem__(self, idx):\n",
    "#         ...\n",
    "#         image = np.load(img_name).astype(np.float32) # e.g. (64,64)\n",
    "#         if image.ndim == 2: # (H, W)\n",
    "#             image = np.expand_dims(image, axis=0) # (1, H, W)\n",
    "#         image = torch.from_numpy(image) # Convert to tensor\n",
    "#         ...\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image) # Apply transforms to tensor\n",
    "#         return image, label\n",
    "\n",
    "# Then transforms can be:\n",
    "train_transforms = transforms.Compose([\n",
    "    # transforms.ToTensor(), # Use if loading PIL images or HWC numpy. Not needed if data is already C,H,W tensor or loaded as such.\n",
    "    transforms.Resize((128, 128), antialias=True), # Or your target size\n",
    "    # transforms.RandomHorizontalFlip(), # Example augmentation\n",
    "    # transforms.RandomRotation(10),   # Example augmentation\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # For single channel. If 3 channels (e.g. RGB-like): [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "                                                # Calculate these from your dataset!\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128), antialias=True),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "# Ensure your MedicalImageDataset is compatible with these transforms.\n",
    "# Specifically, the output of __getitem__ before transform should be what transforms expect.\n",
    "# If MedicalImageDataset returns a NumPy array (e.g. (1, H, W)), transforms.ToTensor() is not needed\n",
    "# if you convert to tensor inside __getitem__. If it returns PIL, ToTensor() is good.\n",
    "\n",
    "# Let's assume MedicalImageDataset is modified to return a tensor (C,H,W)\n",
    "# and image_dir points to where .npy files are.\n",
    "# For dummy data, we need to ensure dummy .npy files exist.\n",
    "DUMMY_IMG_DIR = './data/dummy_images/'\n",
    "os.makedirs(DUMMY_IMG_DIR, exist_ok=True)\n",
    "if not os.listdir(DUMMY_IMG_DIR): # Create dummy files if dir is empty\n",
    "    print(\"Creating dummy .npy files for demonstration...\")\n",
    "    for fname in metadata_df['image_id']:\n",
    "       # Assuming 2D slices, (1, H, W) where H=W=64\n",
    "       np.save(os.path.join(DUMMY_IMG_DIR, fname), np.random.rand(1, 64, 64).astype(np.float32))\n",
    "\n",
    "train_dataset = MedicalImageDataset(dataframe=train_df, image_dir=DUMMY_IMG_DIR, transform=train_transforms)\n",
    "val_dataset = MedicalImageDataset(dataframe=val_df, image_dir=DUMMY_IMG_DIR, transform=val_test_transforms)\n",
    "test_dataset = MedicalImageDataset(dataframe=test_df, image_dir=DUMMY_IMG_DIR, transform=val_test_transforms)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Visualize a sample batch (optional, but good for verification)\n",
    "def show_batch(data_loader, n_images=5):\n",
    "    images, labels = next(iter(data_loader))\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 3))\n",
    "    for i in range(n_images):\n",
    "        img = images[i].squeeze().cpu().numpy() # Remove channel dim if 1, and move to CPU\n",
    "        # Denormalize for visualization if needed: img = img * 0.5 + 0.5\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"Label: {labels[i].item():.0f}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample batch from training loader:\")\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model Implementation\n",
    "\n",
    "Here, we'll define a simple Convolutional Neural Network (CNN) for our medical image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A simple CNN for 2D image classification\n",
    "# Input: (Batch, 1, ImageSize, ImageSize), e.g. (32, 1, 128, 128)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1): # num_classes=1 for binary classification with sigmoid\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1), # Output: (16, 128, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: (16, 64, 64)\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1), # Output: (32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: (32, 32, 32)\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1), # Output: (64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: (64, 16, 16)\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size after conv layers\n",
    "        # For input (1, 128, 128), after 3 maxpools by 2: 128 / (2*2*2) = 128 / 8 = 16\n",
    "        self.flattened_size = 64 * 16 * 16 \n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes) # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x # Raw logits\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN(num_classes=1).to(device) # num_classes=1 for binary (output will be passed to sigmoid)\n",
    "print(model)\n",
    "\n",
    "# Test with a dummy input\n",
    "dummy_input = torch.randn(2, 1, 128, 128).to(device) # Batch size 2, 1 channel, 128x128\n",
    "output = model(dummy_input)\n",
    "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "print(f\"Model output shape: {output.shape}\") # Should be (2, 1) for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training and Evaluation\n",
    "\n",
    "We'll now set up the training loop, define our loss function (e.g., Binary Cross-Entropy with Logits for binary classification), an optimizer (e.g., Adam), and a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, optimizer, and scheduler\n",
    "criterion = nn.BCEWithLogitsLoss() # Includes sigmoid, expects raw logits from model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # Optional\n",
    "\n",
    "NUM_EPOCHS = 10 # Adjust as needed\n",
    "\n",
    "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for inputs, labels in tqdm(data_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1) # Ensure labels are [B, 1] for BCEWithLogitsLoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Store predictions and labels for epoch metrics\n",
    "        preds_proba = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        all_predictions.extend(preds_proba)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    all_predictions = np.array(all_predictions).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Threshold probabilities for binary classification metrics\n",
    "    # You might want to tune this threshold based on validation set performance (e.g., using ROC curve)\n",
    "    threshold = 0.5 \n",
    "    binary_preds = (all_predictions >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    precision = precision_score(all_labels, binary_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, binary_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, binary_preds, zero_division=0)\n",
    "    \n",
    "    # AUC requires probabilities if labels are binary\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_predictions) # Use probabilities for AUC\n",
    "    except ValueError: # Handles cases with only one class present in labels\n",
    "        auc = 0.0 if len(np.unique(all_labels)) < 2 else 0.5 # Or handle as appropriate\n",
    "\n",
    "    metrics = {'loss': epoch_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = [] # Store probabilities for AUC\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds_proba = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_predictions.extend(preds_proba)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    all_predictions = np.array(all_predictions).flatten()\n",
    "\n",
    "    threshold = 0.5\n",
    "    binary_preds = (all_predictions >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    precision = precision_score(all_labels, binary_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, binary_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, binary_preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_predictions)\n",
    "    except ValueError:\n",
    "        auc = 0.0 if len(np.unique(all_labels)) < 2 else 0.5\n",
    "\n",
    "\n",
    "    metrics = {'loss': epoch_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
    "    return metrics\n",
    "\n",
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'train_auc': [],\n",
    "           'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    # if scheduler: scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.4f} | Train AUC: {train_metrics['auc']:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}   | Val Acc: {val_metrics['accuracy']:.4f}   | Val AUC: {val_metrics['auc']:.4f}\")\n",
    "\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['train_auc'].append(train_metrics['auc'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['val_auc'].append(val_metrics['auc'])\n",
    "\n",
    "    # Optional: Save best model\n",
    "    # if val_metrics['auc'] > best_val_auc:\n",
    "    #     best_val_auc = val_metrics['auc']\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# Optional: Save the final model\n",
    "# torch.save(model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Performance Metrics\n",
    "\n",
    "Let's visualize the training progress and evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_auc'], label='Train AUC') # Or 'train_acc'\n",
    "plt.plot(history['val_auc'], label='Validation AUC') # Or 'val_acc'\n",
    "plt.title('AUC Over Epochs') # Or 'Accuracy Over Epochs'\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC') # Or 'Accuracy'\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "test_metrics = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Test F1-Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Test AUC: {test_metrics['auc']:.4f}\")\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "model.eval()\n",
    "all_labels_test = []\n",
    "all_preds_test_proba = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Getting Test Preds\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        preds_proba = torch.sigmoid(outputs).cpu().numpy()\n",
    "        all_preds_test_proba.extend(preds_proba)\n",
    "        all_labels_test.extend(labels.cpu().numpy())\n",
    "\n",
    "all_labels_test = np.array(all_labels_test).flatten()\n",
    "all_preds_test_proba = np.array(all_preds_test_proba).flatten()\n",
    "all_preds_test_binary = (all_preds_test_proba >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(all_labels_test, all_preds_test_binary)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Visualization of Results\n",
    "\n",
    "Visualizing the model's predictions can provide insights into its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, data_loader, device, num_images=5, threshold=0.5):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs >= threshold).float()\n",
    "\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = preds.cpu()\n",
    "    probs = probs.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 4))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        img = images[i].squeeze().numpy() # Assuming single channel\n",
    "        # Denormalize if needed: img = img * 0.5 + 0.5\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        true_label = labels[i].item()\n",
    "        pred_label = preds[i].item()\n",
    "        pred_prob = probs[i].item()\n",
    "        axes[i].set_title(f\"True: {true_label:.0f}\\nPred: {pred_label:.0f} ({pred_prob:.2f})\", \n",
    "                          color=(\"green\" if true_label == pred_label else \"red\"))\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(\"Sample Model Predictions on Test Set\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing sample predictions from the test set:\")\n",
    "visualize_predictions(model, test_loader, device, num_images=5)\n",
    "\n",
    "# For heatmaps/attention maps (e.g., Grad-CAM), you would need additional libraries/implementation.\n",
    "# This is a placeholder for where such code would go.\n",
    "# Example (conceptual):\n",
    "# from pytorch_grad_cam import GradCAM\n",
    "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "# target_layers = [model.conv_layers[-1]] # Example: last conv layer\n",
    "# cam = GradCAM(model=model, target_layers=target_layers)\n",
    "# input_tensor = ... # A sample image tensor\n",
    "# grayscale_cam = cam(input_tensor=input_tensor, targets=None) # Or specify target class\n",
    "# visualization = show_cam_on_image(image_np_array, grayscale_cam[0, :], use_rgb=False)\n",
    "# plt.imshow(visualization)\n",
    "# plt.title(\"Grad-CAM\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 5. Challenges & Limitations\n",
    "\n",
    "<!--\n",
    "* **Common Pitfalls:** Discuss potential issues users might encounter (e.g., overfitting, data imbalance, slow training) and how to address them.\n",
    "* **Known Limitations:** Be transparent about the limitations of the presented approach/model (e.g., \"This simple CNN might not perform well on very complex cases,\" \"Doesn't handle 3D context well\").\n",
    "* **Ethical Considerations:** If relevant (especially in medical AI), discuss biases in data, model fairness, interpretability, and responsible AI use.\n",
    "-->\n",
    "\n",
    "**Common Pitfalls and How to Avoid Them:**\n",
    "* **Overfitting:** `[e.g., The model performs well on training data but poorly on validation/test data. Solutions: More data, data augmentation, regularization (Dropout, L2), early stopping, simpler model.]`\n",
    "* **Data Imbalance:** `[e.g., If one class is much more frequent. Solutions: Weighted loss functions, oversampling minority class, undersampling majority class, using metrics like F1-score or AUC over accuracy.]`\n",
    "* **Vanishing/Exploding Gradients:** `[e.g., Especially in deeper networks. Solutions: Proper weight initialization, batch normalization, gradient clipping, using activation functions like ReLU.]`\n",
    "* **Incorrect Preprocessing:** `[e.g., Normalization values incorrect, images not resized consistently. Solution: Double-check preprocessing steps, visualize augmented data.]`\n",
    "\n",
    "**Known Limitations of This Approach:**\n",
    "* `[e.g., The baseline model presented is relatively simple and may not achieve state-of-the-art performance.]`\n",
    "* `[e.g., This tutorial focuses on 2D slices; a full 3D approach might yield better results for volumetric data but is more computationally intensive.]`\n",
    "* `[e.g., The dataset size might be limited for training very deep networks from scratch.]`\n",
    "* `[e.g., Interpretability of the \"black-box\" CNN model can be challenging, though techniques like Grad-CAM can help.]`\n",
    "\n",
    "**Ethical Considerations & Biases (if relevant):**\n",
    "* **Dataset Bias:** `[e.g., If the training data is not diverse (e.g., lacks representation from certain demographics or scanner types), the model may perform poorly on underrepresented groups. Strive for diverse and representative datasets.]`\n",
    "* **Algorithmic Bias:** `[e.g., The model might inadvertently learn spurious correlations, leading to biased predictions.]`\n",
    "* **Clinical Validation:** `[e.g., Any AI model for medical use requires rigorous clinical validation before deployment. This tutorial is for educational purposes only.]`\n",
    "* **Accountability & Transparency:** `[e.g., Who is responsible if the model makes an error? How can decisions be explained to clinicians and patients?]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 6. Further Reading & References\n",
    "\n",
    "<!--\n",
    "* **Research Papers:** Link to seminal papers or recent advancements.\n",
    "* **Books:** Suggest relevant textbooks.\n",
    "* **Online Resources:** Blogs, courses, documentation.\n",
    "* **Related Implementations:** Links to GitHub repos with similar or more advanced models.\n",
    "-->\n",
    "\n",
    "**Research Papers:**\n",
    "* `[e.g., Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv:1505.04597 - Link]`\n",
    "* `[e.g., Setio, A. A. A., et al. (2017). Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in CT images: The LUNA16 challenge. Medical Image Analysis. - Link]`\n",
    "\n",
    "**Books:**\n",
    "* `[e.g., \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville]`\n",
    "* `[e.g., \"Medical Imaging with Deep Learning\" by [Author Name(s)] - if a specific one exists]`\n",
    "\n",
    "**Online Resources & Courses:**\n",
    "* `[e.g., PyTorch Official Tutorials: https://pytorch.org/tutorials/]`\n",
    "* `[e.g., fast.ai course: https://course.fast.ai/]`\n",
    "* `[e.g., Stanford CS231n: Convolutional Neural Networks for Visual Recognition: http://cs231n.stanford.edu/]`\n",
    "* `[e.g., Grand Challenge (for datasets and challenges): https://grand-challenge.org/]`\n",
    "\n",
    "**Related Open-Source Implementations:**\n",
    "* `[e.g., Link to a relevant GitHub repository for U-Net implementations]`\n",
    "* `[e.g., Link to MONAI (Medical Open Network for AI) library: https://monai.io/]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on completing the tutorial!** We hope you found it informative and useful.\n",
    "If you have any questions or feedback, please feel free to reach out or leave a comment on the YouTube video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
